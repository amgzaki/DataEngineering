# Sparkify PostgreSQL ETL
This is a project from Udacity Data_Engineering Non-degress course

## Overview
Sparkify is a startup in the music industry. The goal of this modelling exercise is to collect data on songs and user activities to analyze what songs users are listening to. The data is in JSON format, which gets ingested into a PostgreSQL DB and is a subset from the [Million Song Dataset](http://millionsongdataset.com/)

## Project Desciption
Uses a star scehema with 1 fact table and 4 dimension tables. Python is used as a wrapper for PostrgreSQL and to trigger the ETL pipeline.
  - `sql_queries.py`: has all the drop/create/insert Python Wrapper statements for the 5 PostrgreSQL tables.
  - `create_tables.py`: calls the queries in `sql_queries.py` to drop and create all the tables.
  - `etl.ipynb`: tests manually single entries, calls the `insert` statements from the `sql_queries.py`.
  - `etl.py`: ETL pipeline to iterate through and insert the Song and Log data files. 
  - `test.ipynb`: uses IPython extensions to directly run PostgreSQL queries on inserted data.

## Data
- `Song Dataset`: Each JSON file contains metadata about a song and the artist of the song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.
    -     song_data/A/B/C/TRABCEI128F424C983.json 
    Here is an example of what a song file  looks like. 
    -      {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
 -   `Log Dataset`: log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. The log files are partitioned by year and month. Here is a sample
     -     log_data/2018/11/2018-11-12-events.json

## Database
Star schema with 1 Fact table in the center and 4 surrounding dimension tables.
The fact table has foreign keys each refrence the primary key in each of the dimension tables. This allows us to de-normalize our tables easier, simplify our queries, and do fast aggregations. 
![](https://github.com/amgzaki/DataEngineering/blob/master/1_Data_Modeling_with_PostgreSQL/Sparkify%20ERD.png)

#### Fact Table
**songplays**
- songplay_id int PRIMARY KEY 
- start_time timestamp REFERENCES time
- user_id int REFERENCES users
- level text 
- song_id text REFERENCES songs
- artist_id text REFERENCES artists
- session_id int
- location text
- user_agent text

#### Dimension Tables
**users**
- user_id int PRIMARY KEY
- first_name text 
- last_name text 
- gender text 
- level text

**songs**
- song_id text PRIMARY KEY
- title text
- artist_id text
- year int
- duration float

**artists**
- artist_id text PRIMARY KEY
- name text
- location text
- latitude float 
- longitude float

**time**
- start_time timestamp PRIMARY KEY
- hour int
- day int
- week int
- month text
- year int
- weekday text   

## Execution Steps
1. On the create_tables.py, modify your PostgreSQL database connection strings
2. Run the `create_tables.py`. This will create the database, drop the tables if they exist, and creates the 5 needed tables
3. Run the etl.py or scehdule a cron. This will trigger the ETL pipeline transaction. 

